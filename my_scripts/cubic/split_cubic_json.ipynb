{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "cubic_data = \"../cubic_data_split.json\"\n",
    "\n",
    "# Open the JSON file\n",
    "with open(cubic_data, \"r\") as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data[\"test\"]\n",
    "data_val = data[\"validation\"]\n",
    "data_train = data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14421\n",
      "Number of test samples: 1000\n",
      "Number of validation samples: 1000\n",
      "total: 16421\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples: {}\".format(len(data_train)))\n",
    "print(\"Number of test samples: {}\".format(len(data_test)))\n",
    "print(\"Number of validation samples: {}\".format(len(data_val)))\n",
    "print(\"total: {}\".format(len(data_train) + len(data_test) + len(data_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = data_train + data_test + data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_split_data(data, train,test,val,seed=42):\n",
    "    datac = data.copy()\n",
    "    random.seed(seed)\n",
    "    random.shuffle(datac)\n",
    "    train_data = datac[:train]\n",
    "    test_data = datac[train:train+test]\n",
    "    val_data = datac[train+test:train+test+val]\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train, new_test, new_val = random_split_data(whole_data, 1442, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, path):\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic_data_new = \"../cubic_data_split_010.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to the folder\n",
    "dataset_pbc = \"../../dataset_pbc/datas\"\n",
    "\n",
    "# Get the list of files in the folder\n",
    "files = os.listdir(dataset_pbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17418"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14421\n",
      "Number of test samples: 1000\n",
      "Number of validation samples: 1000\n"
     ]
    }
   ],
   "source": [
    "def match_file(files, data):\n",
    "    output = []\n",
    "    for file in files:\n",
    "        mp_id = file.split(\".\")[0]\n",
    "        if mp_id in data:\n",
    "            output.append(file)\n",
    "    return output\n",
    "\n",
    "train_files = match_file(files, data_train)\n",
    "test_files = match_file(files, data_test)\n",
    "val_files = match_file(files, data_val)\n",
    "print(\"Number of training samples: {}\".format(len(train_files)))\n",
    "print(\"Number of test samples: {}\".format(len(test_files)))\n",
    "print(\"Number of validation samples: {}\".format(len(val_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_files(files, source_folder, destination_folder):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Copy the files to the destination folder\n",
    "    for file in files:\n",
    "        source_path = os.path.join(source_folder, file)\n",
    "        destination_path = os.path.join(destination_folder, file)\n",
    "        shutil.copyfile(source_path, destination_path)\n",
    "# Specify the path to the source folder\n",
    "source_folder = \"../../dataset_pbc/datas\"\n",
    "\n",
    "# Specify the path to the destination folder\n",
    "destination_folder = \"../../dataset_pbc/splits\"\n",
    "\n",
    "copy_files(train_files, source_folder, os.path.join(destination_folder, \"train\"))\n",
    "copy_files(test_files, source_folder, os.path.join(destination_folder, \"test\"))\n",
    "copy_files(val_files, source_folder, os.path.join(destination_folder, \"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = \"../../dataset_pbc/datas_filtered\"\n",
    "copy_files(train_files + test_files + val_files, source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14421\n",
      "Number of test samples: 1000\n",
      "Number of validation samples: 1000\n",
      "total: 16421\n",
      "Number of dataset all samples: 16421\n"
     ]
    }
   ],
   "source": [
    "# file len check\n",
    "\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder\n",
    "dataset_all = \"../../dataset_pbc/datas_filtered\"\n",
    "dataset_train = \"../../dataset_pbc/splits/train\"\n",
    "dataset_test = \"../../dataset_pbc/splits/test\"\n",
    "dataset_val = \"../../dataset_pbc/splits/val\"\n",
    "\n",
    "print(\"Number of training samples: {}\".format(len(os.listdir(dataset_train))))\n",
    "print(\"Number of test samples: {}\".format(len(os.listdir(dataset_test))))\n",
    "print(\"Number of validation samples: {}\".format(len(os.listdir(dataset_val))))\n",
    "print(\"total: {}\".format(len(os.listdir(dataset_all))))\n",
    "\n",
    "print(\"Number of dataset all samples: {}\".format(len(os.listdir(dataset_all))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infGCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
